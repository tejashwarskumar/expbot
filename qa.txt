How many experiments led to an increase in premium trial initiations?
4 experiments led to an increase in premium trial initiations.
How many copy experiments were rolled out in Sep?
7 copy experiments were rolled out in Sep.
What is the average experiment length/duration for an experiment to see statistically significant results?
The average experiment length/duration for an experiment to see statistically significant results is 14 days.
What are the commonly used success metrics used by premium experimenters?
The commonly used success metrics used by premium experimenters are "Users clicking on the button" and "Users who started the trial".
What are the guidelines for identifying relevant success metrics for my experiment?
The guidelines for identifying relevant success metrics for your experiment are that they should align with business goals, be quantifiable and measurable, be sensitive to changes and consistent across flights.
What are the most common guardrail conditions for growth experiments?
The common guardrail conditions for growth experiments are that the Core metrics should not exhibit any regressions in product features.
I want to design an experiment to test the performance of x copy variants. What should be my success metrics?
To design an experiment to test the performance of x copy variants the success metrics should be Click Through Rate for the feature copy.
I ran an experiment, with 8 metrics showing no negative movements. Can I call my experiment successful?
Even if the metrics show no negative movements, the experiment can be considered as successful only if the movement in the metrics is statistically significant.
What is the guidance for when an experiment's success metrics are close to being statistically significant but the conversion rate is low?
The general guidance for when an experiment shows statistically significant results but has a low conversion rate is to increase the population level for the experiment to get more meaningful results, as the current population size is too small to draw accurate conclusions. Increasing the population size would help in reaching the desired number of users for accurate analysis. Alternatively, it can be recommended that the experiment could start with a larger population by default and use the required traffic filters to remove a large chunk of the population, so that the relevant users are shown the experiment. The experiment could also start with a higher percentage of the population to begin with and then balance the policy and risk factors based on the results to avoid running experiments for extended periods without meaningful data.
What are the allocation percentages at which experiments should be rolled out?
For a feature experiment, the guidance is to start at 10% and then iterate up to 25% and finally 50% between the Treatment and Control groups. For a campaign involving coachmarks, modals etc., they can be directly rolled out to Ring 4 at 50% allocation to capture the maximum number of users as campaigns generally do not affect the product performance. But, it is important to monitor the Performance and Memory xCards during the pre-rollout phase to detect any performance or memory leak issues.
Are Experiments supported in all rings?
The Experimentation platform only supports Experiments on Ring 3 and Ring 4. Ring 2 and Ring 3.9 experiments are not encouraged given low traffic that makes it much harder to obtain insights and given company only audience introduce bias that is not representative of our larger company product audience. Rings 0, 1 are not supported due PR gates and automation that are impacted by traffic split in lower rings. Please notice that low rings are not big enough to derive significant metric movements in the scorecards. Ring 3 and Ring 4 have a much bigger user base which results in more accurate Experimentation results. If you're planning to run an Experiment on Ring 4, please reach out to the Experimentation team on the channel or in the [Experimentation Office Hours](https://mocklink.com/Master). If you are planning to run an experiment with changes in the UI, please have the feature reviewed with the Design team prior to running the experiment.
I want to use an experiment to help me troubleshoot an issue, can I use Ring 0 or Ring 1 for this effort ?
Due to PR gates and a series of automations especially in R0 and R1, experimentation or any traffic split are not authorized in those rings.
Are Experiments supported in NorthStar?
Yes experiments are supported in NorthStar. Please refer to this wiki for more information: https://mocklink.com/northstar 
Are Experiments supported in Government Clouds (GCCH/GCC/DoD)?
No, we do not support Experimentation in Government Clouds.
Are Mobile Experiments supported?
Yes, there mobile experiments are supported for iOS and Android. Windows Phone is not supported.
What's the difference between Experiments and Rollouts?
Wiki entry [here](https://mocklink.com/How-to-create-an-Experiment). For more information about rollouts, go to https://mocklink.com/flight 
Can I run an experiment on a specific list of User Ids or Tenant Ids?
No, you do not want to make decisions based on a subset of the population that might differ from your release target audience. If you have a large enough tenant that needs to be filtered on a given experiment please follow the instructions on [Tenant based filter set-up](https://mocklink.com/Tenant-based-filter-set-up). You can also follow rollout instructions to enable the experience on a specific tenant or user.
How to ship feature flags after running a forward experiment
Update Platform config files (TMP or TSW). Wait for new build to read config files and fully update the main Platform rollouts (i.e. reach 100%). Stop forward experiment.
When converting an Experiment to a Staged Rollout, when do I stop the Experiment?
If you are converting a running Experiment to a staged rollout, you should stop the Experiment after the Rollout has reached 100%. This will ensure that any users that were previously allocated to the Experiment continue to see a consistent experience as the rollout propagates to all users.
When creating a Staged Rollout for a running an Experiment (to convert into a rollout), how the two interact?
When converting a a running Experiment to a staged rollout, you should ensure that the configuration of the rollout is the exact same that the original experiment. Lets say your experiment currently targets 10% with treatment and your target % rollout (for example is 30%). The 30% in this case might include some users in the experiment, although the experiment will be protected from interference since it takes precedence, therefore your rollout will effectively make a population of 30-40% now in the treatment.
What is an SRM?
SRM stands for "Sample Ratio Mismatch". Please refer to the "[Understanding SRM alerts](https://mocklink.com/Understanding-SRM-alerts)" page
Can I run Desktop experiments on a particular Desktop build?
We are currently not supporting Desktop experiments on a particular Desktop build
How do I fill the EXP plan when running an experiment with two or more treatments ?
On the [EXP Plan](https://mocklink.com/forms) in the question "Traffic Allocation Percent" you should choose: Other and fill the % used for each treatment (e.g. A/B/C would be 33).
What do I need do to re-run an experiment?
A new ADO item is needed for each experiment. When re-running existing experiment, please clone the existing ADO item for the EXP plan and get the new plan approved like before. Then reach out to Exp DRI to get the exp restarted.
Is my scorecard impacted when multiple experiments run on the same set of users?
The product Experimentation platform does user-based randomization when an experiment is created, finding a similar set of users for Treatment and Control. When there's multiple experiments running concurrently, your feature should not be impacted negatively by other features, since there's an equal distribution of the other Treatment and Control users from other experiments in the Treatment and Control groups in your experiment. For more information, please review the A&E wiki entry regarding [Concurrent experiments and interaction detection](https://mocklink.com/Concurrent-experiments-and-interaction-detection).
Why are the VR (Variance Reduction) metrics missing from my scorecard?
This happens when the Retro-AA jobs are delayed or have failed. Please reach out to the Experimentation DRI if you need the VR metrics to get retriggered. Note that once the scorecard missing the VR metrics is generated, it'd require to retrigger the xcard manually to have the VR metrics show up. Once the Retro-AA job is completed, any future scorecards will have VR metrics available.
I am not seeing my Experiment Plan being posted or ADO work-item being created. Is it a known issue?
When adding the link to the parent VSO work item, please make sure that the parent url does not have an extra "/" in the end. The link should look like this: https://mocklink.com/workitems/edit/**420**
How do I find in which Ring a specific tenant is located ?
Go to https://mocklink.com/audience-prod.json
Any questions related to Shipping Scorecards?
Go to https://mocklink.com/exp/faq 
Issues in creation of Northstar component of Ship Scorecard?
Please reach out to Experimentation team in [Company product > Experimentation and Staged Rollout channel](https://mocklink.com/channel) Channel for support
Can I restart an Platform experiment with the same seed ensuring the same user base split for the feature?
Yes. the allocation buckets and salt would never change for an experiment unless "Reshuffle and Restart" is clicked on the Platform UI. Traffic changes, filter changes, adding/removing treatments can be done with the restart without a change in the seed.
Is ClientVersion filter supported for Web/Desktop Experiments ?
ClientVersion Filter - is not supported via JSON checked in as part of the Experiment PR or via Platform UI (as on 9/8/2021) and should not be used to target the audience of an experiment. Support has not been implemented in the desktop client, any experiment with ClientVersion set do not collect any metrics from desktop, which prevents key metrics such as ALT, crash, etc. to get measured.
Tenant Lists in Platform FAQ
[here](https://mocklink.com/Tenant-Lists)
What UserID is utilized by product as randomization unit in Platform?
product for Work (TFW) experiments and rollouts are currently randomized on AAD user id. product for Life (TFL) would focus on MSA ids.
I need DRI assistance. How can I find the on-call engineer contact?
Find the infrastructure - experimentation DRI in [ICM](https://mocklink.com/oncall)
Can I debug using rollout config IDs in Kusto?
By default rollouts do not stamp telemetry and you should refrain from relying on AppInfo_RolloutIds for Kusto debugging. This column is disabled because of data size concerns. Although you might see events where this column is non-null, they are telemetry events which are coming from specific allowed rollout events to support impression-based or specific scenarios. If your rollout qualifies for allow listing please follow the steps below: Allowlist the Config ID via [TMP exp-configs](https://mocklink.com/exp-configs) for the desired experience under the exp-configs/<experience>/metadata/ folder. Note: Be sure to allowlist only part of the Config ID like so: P-R-<rolloutId> excluding the -<expId>-<variantId> suffix from it to enable single allowlist for multiple iterations of the rollout.
Can I debug a specific build rollout using rollout config IDs in Kusto?
For certain Builds that utilize PDS telemetry, you can follow the guidance provided in https://mocklink.com/tool. Execute in [Web] [Desktop] [cluster('mock.os.net').database('ProductFabric')]. RequestTelemetry | where PlatformConfigIDs contains "P-R-1024" | where EnvironmentName contains "pds" or EnvironmentName contains "pckg" | where TIMESTAMP >= ago(1d) | summarize dcount(UserId) by Audience
Can I start a new Platform only experiment using the same salt which was used in a previous stopped experiment?
Yes, this can be done. Before starting the experiment, experiment team can help in updating the salt for the experiment as the salt of previous experiment. Experimentation team has access to Platform backend api and can update Platform experiment to have the same salt. Reach out to experimentation team with old and new experiment ids.
I can't start my control tower driven experiment. Start button is grayed out. Insufficient traffic remaining.
This isue can be resolved following self served steps given o this wiki. https://mocklink.com/Troubleshooting-Insufficient-traffic
Can I reactivate an Platform PR that was abandoned?
No, once the PR is abandoned, Platform will unlik it so you need to generate a new PR through the Platform UX.
Can I run experiments on product Middletier?
No, we do not support Experimentation on product Middletier.
Can a Kusto query be correlated to an experiment iteration?
Yes. At the top of an Platform experiment page there is information about the current iteration, including buttons to cycle backwards through previous iterations. By clicking the "Show Iteration Changes" button you can see information about when each iteration went live. Using this Kusto query fragment as an example: let ['_expId']='P-E-105047-4-12'; The -12 at the end of the line refers to the iteration of the experiment. You can specify any iteration you want, but if you need all data it's important that the number in the Kusto query matches the latest iteration of the experiment.
I started my experiment but don't see scorecards scheduled?
The scorecards are scheduled in UTC timezone i.e. 1 day of scorecard contains data from UTC 0th hour to UTC 23rd hour. If the experiment was started in between i.e between 0th hour UTC and 23rd hour UTC, the experiment data would be present in Kusto and Interana but would that no be processed into scorecard for the day of the start of the experiment. The first scorecard would be based on the data of the next UTC day from when the experiment was started.
How is experiment supported for MTMA (Multi-Tenant Multi-Account) in T2.X
In Multi-Tenant/multi-Account scenario, one account is as a primary account (based on predefined heuristics) and the primary user is used to make Platform call to fetch the feature flag and build configurations. At Platform level, only primary accounts is used to do the flight assignments and all other secondary users are also part of the same flight at client level. Platform is unaware of secondary accounts logged into the client as there is no Platform fetch happening for other logged in users. By stamping primary user id in a different field on all the telemetry coming from secondary users, we include telemetry received from secondary users in experiment result without having any SRMs. While looking at the telemetry stamped for an experiment for which scorecard is being generated, if PrimaryUserInfoId field has a value then we use this filed as UserInfoId else the existing field UserInfoId is taken. Doing so we include telemetry coming from the client under the experiment in MTMA and non MTMA scenarios. A new segment has been added to the scorecards which are based on MtmaAccounts field which enables us to see the scorecard for different segments depending on the number of accounts logged into the product client. This segment has values as 1,2,3,4,5 and above showing the number of accounts in the segment. [Ref](https://mocklink.com/ref) 
What happens to an MTMA user in an experiment cross-linking T2.X and Mobile?
For MTMA users that are also accessing product on a non-MTMA supported surface (Web, Android or iOS) the behavior will be determined by which account the user is utilizing. Access Web/Android/iOS using the primary account from their Windows/Mac experience - the user will be assigned to the same cohort (control or treatment) that they were previously assigned when using Windows/Mac. Access Web/Android/iOS using one of the secondary accounts from their Windows/Mac experience - the user will be evaluated and if this is the first time the user account is being used while the experiment is active, it will be evaluated as any standard experiment to determine if qualifies to the experiment, if it does it will be assigned a new cohort in the same method that is used to qualify any user joining the experiment for the first time.
How does scorecard works for users that have both MTMA and non-MTMA usage during the experiment ?
User assignment for the scorecard calculations will take the primary user ID into consideration, for all MTMA surface work the utilization of primary and secondary accounts will be assigned to the primary user cohort, for non-MTMA surface the primary user will be the same as the standard user ID. This is done to ensure that the scorecard is displaying the user experience.
I still see experiment ids for a stopped mobile experiment?
Users either launching their app for the first time in a long time or switching tenant can log experiment ids for old experiments. Kindly check if non-start up scenarios are also logged with older ids. Post startup a new Platform fetch happens and should only log active experiments' ids
Why did my experiment stop running?
We auto-stop experiments after 7day. Please check "What is the process to extend an experiment beyond the standard duration?" is you want to avoid the auto-stop.
What is the process to extend an experiment beyond the standard duration?
You can select to run experiments for 7 or 14-days with automatic stop, if you need a different duration period then your path depends on the stage that you are in the experiment: Set up time: path will differ slightly for classic vs simplified: Classic: Adjust your PR to have "IsNonDefaultDuration": true Simplified: In the question "Use non default experimentation duration" you should select Yes. Experiment is already running: Post on the Experimentation channel, requesting the [On Call DRI](https://mocklink.com/oncall) to make the change. Additional caveats: Operational and User Behavior scorecards using xCard will only have data up to 30 days due to retention policy. By asking for an experiment to be set as non-default duration, the experiment will only end when an explicit request from the feature team is made or after a maximum period of one quarter (subject to change in policy) as experiments are not intended to be long running experiences.
Can I experiment in a different Platform ProjectTeam?
Yes, this is possible by crosslinking experiments. You would need to go through the product Experiment process to create Platform experiment in standard Web/Desktop/Andorid/iOS Platform ProjectProduct. Please ensure to set auto-start to false. Once Platform experiment in standard ProjectProduct is/are created, a child experiment from your specific ProjectProduct can be created and cross linked to the other at the point of creation in Platform UI. The cross linking helps make sure the user is the control segment for all experiments and hence the scorecards from standard Platform ProjectProduct can be used to evaluate impact of configs experimented in other Platform ProjectProduct.